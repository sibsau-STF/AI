{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voise recognition via _voise.csv_ file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.064241</td>\n",
       "      <td>0.032027</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>0.090193</td>\n",
       "      <td>0.075122</td>\n",
       "      <td>12.863462</td>\n",
       "      <td>274.402906</td>\n",
       "      <td>0.893369</td>\n",
       "      <td>0.491918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.084279</td>\n",
       "      <td>0.015702</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.067310</td>\n",
       "      <td>0.040229</td>\n",
       "      <td>0.019414</td>\n",
       "      <td>0.092666</td>\n",
       "      <td>0.073252</td>\n",
       "      <td>22.423285</td>\n",
       "      <td>634.613855</td>\n",
       "      <td>0.892193</td>\n",
       "      <td>0.513724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.107937</td>\n",
       "      <td>0.015826</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.083829</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.131908</td>\n",
       "      <td>0.123207</td>\n",
       "      <td>30.757155</td>\n",
       "      <td>1024.927705</td>\n",
       "      <td>0.846389</td>\n",
       "      <td>0.478905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.098706</td>\n",
       "      <td>0.015656</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.072111</td>\n",
       "      <td>0.158011</td>\n",
       "      <td>0.096582</td>\n",
       "      <td>0.207955</td>\n",
       "      <td>0.111374</td>\n",
       "      <td>1.232831</td>\n",
       "      <td>4.177296</td>\n",
       "      <td>0.963322</td>\n",
       "      <td>0.727232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.017798</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.201497</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.247119</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.079146</td>\n",
       "      <td>0.124656</td>\n",
       "      <td>0.078720</td>\n",
       "      <td>0.206045</td>\n",
       "      <td>0.127325</td>\n",
       "      <td>1.101174</td>\n",
       "      <td>4.333713</td>\n",
       "      <td>0.971955</td>\n",
       "      <td>0.783568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.106398</td>\n",
       "      <td>0.016931</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.712812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>5.484375</td>\n",
       "      <td>5.476562</td>\n",
       "      <td>0.208274</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3163</th>\n",
       "      <td>0.131884</td>\n",
       "      <td>0.084734</td>\n",
       "      <td>0.153707</td>\n",
       "      <td>0.049285</td>\n",
       "      <td>0.201144</td>\n",
       "      <td>0.151859</td>\n",
       "      <td>1.762129</td>\n",
       "      <td>6.630383</td>\n",
       "      <td>0.962934</td>\n",
       "      <td>0.763182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131884</td>\n",
       "      <td>0.182790</td>\n",
       "      <td>0.083770</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.832899</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>4.210938</td>\n",
       "      <td>4.203125</td>\n",
       "      <td>0.161929</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3164</th>\n",
       "      <td>0.116221</td>\n",
       "      <td>0.089221</td>\n",
       "      <td>0.076758</td>\n",
       "      <td>0.042718</td>\n",
       "      <td>0.204911</td>\n",
       "      <td>0.162193</td>\n",
       "      <td>0.693730</td>\n",
       "      <td>2.503954</td>\n",
       "      <td>0.960716</td>\n",
       "      <td>0.709570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116221</td>\n",
       "      <td>0.188980</td>\n",
       "      <td>0.034409</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.909856</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>3.679688</td>\n",
       "      <td>3.640625</td>\n",
       "      <td>0.277897</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3165</th>\n",
       "      <td>0.142056</td>\n",
       "      <td>0.095798</td>\n",
       "      <td>0.183731</td>\n",
       "      <td>0.033424</td>\n",
       "      <td>0.224360</td>\n",
       "      <td>0.190936</td>\n",
       "      <td>1.876502</td>\n",
       "      <td>6.604509</td>\n",
       "      <td>0.946854</td>\n",
       "      <td>0.654196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142056</td>\n",
       "      <td>0.209918</td>\n",
       "      <td>0.039506</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.494271</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>2.937500</td>\n",
       "      <td>2.929688</td>\n",
       "      <td>0.194759</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3166</th>\n",
       "      <td>0.143659</td>\n",
       "      <td>0.090628</td>\n",
       "      <td>0.184976</td>\n",
       "      <td>0.043508</td>\n",
       "      <td>0.219943</td>\n",
       "      <td>0.176435</td>\n",
       "      <td>1.591065</td>\n",
       "      <td>5.388298</td>\n",
       "      <td>0.950436</td>\n",
       "      <td>0.675470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143659</td>\n",
       "      <td>0.172375</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.791360</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>3.593750</td>\n",
       "      <td>3.585938</td>\n",
       "      <td>0.311002</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3167</th>\n",
       "      <td>0.165509</td>\n",
       "      <td>0.092884</td>\n",
       "      <td>0.183044</td>\n",
       "      <td>0.070072</td>\n",
       "      <td>0.250827</td>\n",
       "      <td>0.180756</td>\n",
       "      <td>1.705029</td>\n",
       "      <td>5.769115</td>\n",
       "      <td>0.938829</td>\n",
       "      <td>0.601529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165509</td>\n",
       "      <td>0.185607</td>\n",
       "      <td>0.062257</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.227022</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3168 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
       "0     0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
       "1     0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
       "2     0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
       "3     0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
       "4     0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
       "...        ...       ...       ...       ...       ...       ...        ...   \n",
       "3163  0.131884  0.084734  0.153707  0.049285  0.201144  0.151859   1.762129   \n",
       "3164  0.116221  0.089221  0.076758  0.042718  0.204911  0.162193   0.693730   \n",
       "3165  0.142056  0.095798  0.183731  0.033424  0.224360  0.190936   1.876502   \n",
       "3166  0.143659  0.090628  0.184976  0.043508  0.219943  0.176435   1.591065   \n",
       "3167  0.165509  0.092884  0.183044  0.070072  0.250827  0.180756   1.705029   \n",
       "\n",
       "             kurt    sp.ent       sfm  ...  centroid   meanfun    minfun  \\\n",
       "0      274.402906  0.893369  0.491918  ...  0.059781  0.084279  0.015702   \n",
       "1      634.613855  0.892193  0.513724  ...  0.066009  0.107937  0.015826   \n",
       "2     1024.927705  0.846389  0.478905  ...  0.077316  0.098706  0.015656   \n",
       "3        4.177296  0.963322  0.727232  ...  0.151228  0.088965  0.017798   \n",
       "4        4.333713  0.971955  0.783568  ...  0.135120  0.106398  0.016931   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "3163     6.630383  0.962934  0.763182  ...  0.131884  0.182790  0.083770   \n",
       "3164     2.503954  0.960716  0.709570  ...  0.116221  0.188980  0.034409   \n",
       "3165     6.604509  0.946854  0.654196  ...  0.142056  0.209918  0.039506   \n",
       "3166     5.388298  0.950436  0.675470  ...  0.143659  0.172375  0.034483   \n",
       "3167     5.769115  0.938829  0.601529  ...  0.165509  0.185607  0.062257   \n",
       "\n",
       "        maxfun   meandom    mindom    maxdom   dfrange   modindx   label  \n",
       "0     0.275862  0.007812  0.007812  0.007812  0.000000  0.000000    male  \n",
       "1     0.250000  0.009014  0.007812  0.054688  0.046875  0.052632    male  \n",
       "2     0.271186  0.007990  0.007812  0.015625  0.007812  0.046512    male  \n",
       "3     0.250000  0.201497  0.007812  0.562500  0.554688  0.247119    male  \n",
       "4     0.266667  0.712812  0.007812  5.484375  5.476562  0.208274    male  \n",
       "...        ...       ...       ...       ...       ...       ...     ...  \n",
       "3163  0.262295  0.832899  0.007812  4.210938  4.203125  0.161929  female  \n",
       "3164  0.275862  0.909856  0.039062  3.679688  3.640625  0.277897  female  \n",
       "3165  0.275862  0.494271  0.007812  2.937500  2.929688  0.194759  female  \n",
       "3166  0.250000  0.791360  0.007812  3.593750  3.585938  0.311002  female  \n",
       "3167  0.271186  0.227022  0.007812  0.554688  0.546875  0.350000  female  \n",
       "\n",
       "[3168 rows x 21 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('voice.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\User\\.conda\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User\\.conda\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User\\.conda\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User\\.conda\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User\\.conda\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User\\.conda\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\User\\.conda\\envs\\deeplearning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User\\.conda\\envs\\deeplearning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User\\.conda\\envs\\deeplearning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User\\.conda\\envs\\deeplearning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User\\.conda\\envs\\deeplearning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User\\.conda\\envs\\deeplearning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "\n",
    "def init_model(column_count):\n",
    "    model = Sequential([\n",
    "        Dense(units=column_count, input_shape=(column_count,), activation='sigmoid'),\n",
    "        Dense(units=5, kernel_initializer = 'uniform', activation='sigmoid', use_bias=True),\n",
    "        Dropout(0.04),\n",
    "        Dense(units=5, kernel_initializer = 'uniform', activation='sigmoid', use_bias=True),\n",
    "        Dropout(0.01),\n",
    "        Dense(units=4, kernel_initializer = 'uniform', activation='sigmoid', use_bias=True),\n",
    "        Dense(units=2, activation='softmax')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_44 (Dense)             (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 5)                 35        \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 4)                 24        \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 2)                 10        \n",
      "=================================================================\n",
      "Total params: 141\n",
      "Trainable params: 141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adadelta\n",
    "\n",
    "model = init_model(6)\n",
    "#model.build(20)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#opt = SGD(\n",
    "#    learning_rate=0.02,\n",
    "#    decay=0.005,\n",
    "#)\n",
    "opt = Adadelta()\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model.save('voicegender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image as k_image\n",
    "\n",
    "def rand_dataset(dataset_x=[], dataset_y=[], seed=37):\n",
    "    random.seed(seed)\n",
    "    random.shuffle(dataset_x)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(dataset_y)\n",
    "    return dataset_x, dataset_y\n",
    "\n",
    "def getData(dataset):\n",
    "    return dataset[['meanfun', 'Q25', 'Q75', 'IQR', 'meanfreq', 'meandom']]\n",
    "\n",
    "def load_dataset(path, trainset_p=0.75, trainset_partition=0.75):\n",
    "    dataset = pd.DataFrame(pd.read_csv(path, index_col=1))\n",
    "    print(dataset.shape)\n",
    "    dataset_x = [list(), list()]\n",
    "    gender = list(dataset['label'])\n",
    "    dataset = dataset.drop(['label'], axis=1)\n",
    "    dataset = getData(dataset)\n",
    "    print(dataset.shape)\n",
    "    # print(gender[0:10])\n",
    "    # print(dataset)\n",
    "    for i in range(dataset.shape[0]):\n",
    "        row = dataset.iloc[i].to_list()\n",
    "        if gender[i] == 'male':\n",
    "            dataset_x[0].append(row) # male\n",
    "        else:\n",
    "            dataset_x[1].append(row) # female\n",
    "    \n",
    "    trainset_size = (int(len(dataset_x[0]) * trainset_partition), int(len(dataset_x[1]) * trainset_partition))\n",
    "    \n",
    "    trainset_x = dataset_x[0][:trainset_size[0]] + dataset_x[1][:trainset_size[1]]\n",
    "    trainset_y = \\\n",
    "        [[1, 0] for i in range(trainset_size[0])] + \\\n",
    "        [[0, 1] for i in range(trainset_size[1])]\n",
    "    trainset_x, trainset_y = rand_dataset(trainset_x, trainset_y)\n",
    "    \n",
    "    testset_x = dataset_x[0][trainset_size[0]:] + dataset_x[1][trainset_size[1]:]\n",
    "    testset_y = \\\n",
    "        [[1, 0] for i in range(len(dataset_x[0]) - trainset_size[0])] + \\\n",
    "        [[0, 1] for i in range(len(dataset_x[1]) - trainset_size[1])]\n",
    "    testset_x, testset_y = rand_dataset(testset_x, testset_y)\n",
    "    # print(trainset_x, trainset_y)\n",
    "    # print(testset_x, testset_y)\n",
    "    return (np.array(trainset_x), np.array(trainset_y)), (np.array(testset_x), np.array(testset_y))\n",
    "\n",
    "def train_model(model, epochs, cycles, trainset, testset):\n",
    "    for i in range(cycles):\n",
    "        print('########////{0}\\\\\\\\\\\\\\\\########'.format(i))\n",
    "        history = model.fit(trainset[0], trainset[1], \n",
    "            validation_data = (testset[0], testset[1]), \n",
    "            validation_steps = 5,\n",
    "            steps_per_epoch = len(trainset[0]),\n",
    "            epochs = epochs)\n",
    "        model.save('voicegender')\n",
    "        print('########\\\\\\\\\\\\\\\\{0}////########'.format(i))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_44 (Dense)             (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 5)                 35        \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 4)                 24        \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 2)                 10        \n",
      "=================================================================\n",
      "Total params: 141\n",
      "Trainable params: 141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(3168, 20)\n",
      "(3168, 6)\n",
      "Введите количество эпох обучения: 10\n",
      "Введите количество циклов обучения: 8\n",
      "########////0\\\\\\\\########\n",
      "Train on 2376 samples, validate on 792 samples\n",
      "Epoch 1/10\n",
      "2376/2376 [==============================] - 16s 7ms/step - loss: 0.6931 - accuracy: 0.5002 - val_loss: 0.1386 - val_accuracy: 2.5000\n",
      "Epoch 2/10\n",
      "2376/2376 [==============================] - 15s 6ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.1386 - val_accuracy: 2.5000\n",
      "Epoch 3/10\n",
      "2376/2376 [==============================] - 15s 6ms/step - loss: 0.6931 - accuracy: 0.4999 - val_loss: 0.1386 - val_accuracy: 2.5000\n",
      "Epoch 4/10\n",
      "2376/2376 [==============================] - 15s 6ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.1386 - val_accuracy: 2.5000\n",
      "Epoch 5/10\n",
      "2376/2376 [==============================] - 9s 4ms/step - loss: 0.6931 - accuracy: 0.5001 - val_loss: 0.1386 - val_accuracy: 2.5000\n",
      "Epoch 6/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5004 - val_loss: 0.1386 - val_accuracy: 2.5000 - ETA: 5s - loss: 0.6931 - accuracy: 0.50 - ETA: 4s - loss: 0.6931 - accuracy:  - ETA: 4s - loss: 0.6931 - accuracy - ETA: 4s - los - ETA: 3s - loss: 0.6 - ETA: 3s - ETA: 0s - loss: 0.6931 - accuracy\n",
      "Epoch 7/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.1386 - val_accuracy: 2.5000: 0s\n",
      "Epoch 8/10\n",
      "2376/2376 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5000 ETA: 0s - loss: 0.6931 -  - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.1386 - val_accuracy: 2.5000\n",
      "Epoch 9/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5002 - val_loss: 0.1386 - val_accuracy: 2.5000: 3s - loss: 0.6931 - accuracy: 0. - ETA:  - ETA: 2s - loss: 0.6931  - ETA: 1s - loss: 0.6 - ETA: 1s - loss: 0.6931 - accura - ETA: 1s - loss: 0.693 - ETA: 0s - loss: 0\n",
      "Epoch 10/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5001 - val_loss: 0.1386 - val_accuracy: 2.5000 - l - ETA: 1s - loss: 0.6931 - accuracy: 0.50 - ETA: 1s - loss: 0.6931 - ac - ETA: 0s - loss:\n",
      "########\\\\\\\\0////########\n",
      "########////1\\\\\\\\########\n",
      "Train on 2376 samples, validate on 792 samples\n",
      "Epoch 1/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5001 - val_loss: 0.1386 - val_accuracy: 2.5000\n",
      "Epoch 2/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5002 - val_loss: 0.1386 - val_accuracy: 2.5000: 6s - loss: 0.693 - ETA: 6s - loss: 0.6931 - accura - ETA: 5s - loss: 0.6931 - accu - ETA: \n",
      "Epoch 3/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5002 - val_loss: 0.1386 - val_accuracy: 2.5000ra\n",
      "Epoch 4/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5006 - val_loss: 0.1386 - val_accuracy: 2.5000accuracy: 0.\n",
      "Epoch 5/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5004 - val_loss: 0.1386 - val_accuracy: 2.5000 - loss: 0.6931 - accura - ETA: 4s - loss: 0.6931 - accuracy: 0. - ETA: 3s - loss: 0.6 - ETA: 3s - loss: 0.6931 - ac - ETA: 2s - loss: 0.6931 - accura - ETA: 2s - los - ETA: 1s - loss: 0 - E - ETA: 0s - loss: 0.6931 - accuracy: \n",
      "Epoch 6/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5001 - val_loss: 0.1386 - val_accuracy: 2.5000s: 0.6\n",
      "Epoch 7/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5003 - val_loss: 0.1386 - val_accuracy: 2.5000\n",
      "Epoch 8/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.1386 - val_accuracy: 2.5000\n",
      "Epoch 9/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.1386 - val_accuracy: 2.5000 0.6931 - accuracy - ETA: 3s - l - ETA: 1s - loss: 0.6931  - ETA: 1s - loss: 0.6931 - accuracy: 0.50 - ETA: 1s - loss: 0.6931 - accuracy: 0. - ETA: 1s - loss: 0.6931 -  - ETA: 0s - loss: 0.6931  - ETA: 0s - loss: 0.693\n",
      "Epoch 10/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5002 - val_loss: 0.1386 - val_accuracy: 2.5000 - - ETA: 0s - loss: 0.6931 \n",
      "########\\\\\\\\1////########\n",
      "########////2\\\\\\\\########\n",
      "Train on 2376 samples, validate on 792 samples\n",
      "Epoch 1/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5002 - val_loss: 0.1386 - val_accuracy: 2.50000s - loss: 0\n",
      "Epoch 2/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5002 - val_loss: 0.1386 - val_accuracy: 2.5000 - ETA: 1s - loss: 0.6931 - accuracy: 0. - ETA: 0s - loss: 0 - ETA: 0s - loss: 0.6931 - \n",
      "Epoch 3/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5002 - val_loss: 0.1386 - val_accuracy: 2.5000\n",
      "Epoch 4/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5001 - val_loss: 0.1386 - val_accuracy: 2.50001 - accuracy: 0.\n",
      "Epoch 5/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5001 - val_loss: 0.1386 - val_accuracy: 2.5000 - loss: 0.6931 - accuracy:  - ETA: 1s - loss: 0.6931 - accu - ETA: 1s - loss: 0 - ETA: 0s - loss: 0.693 - ETA: 0s - loss: 0.6931 - accuracy: 0.\n",
      "Epoch 6/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.1386 - val_accuracy: 2.5000TA: 0s - loss: 0.6931 - accuracy: \n",
      "Epoch 7/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5001 - val_loss: 0.1386 - val_accuracy: 2.50001s - loss: 0.6931 - accuracy: 0.50 - ETA: 1s - loss: 0.6931 - accura - ETA: 1s - loss: 0.6931  - ETA: 0s - loss: 0.6931 -  - ETA: 0s - loss: 0.6\n",
      "Epoch 8/10\n",
      "2376/2376 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5001 - ETA: 0s - loss: 0.6931 - accura - ETA: 0s - loss: 0.6931 - accuracy - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5001 - val_loss: 0.1386 - val_accuracy: 2.5000\n",
      "Epoch 9/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5001 - val_loss: 0.1386 - val_accuracy: 2.5000oss: 0.6931 - accuracy:  - ETA: 2s - l - ETA: 2s - loss: 0 - ETA: 0s - loss: 0.6931 \n",
      "Epoch 10/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.1386 - val_accuracy: 2.5000TA - ETA: 1s - - ETA: 0s - loss: 0.6931 - \n",
      "########\\\\\\\\2////########\n",
      "########////3\\\\\\\\########\n",
      "Train on 2376 samples, validate on 792 samples\n",
      "Epoch 1/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.1386 - val_accuracy: 2.86621s - loss: 0.6931 - accuracy - ETA: 0s - loss: 0\n",
      "Epoch 2/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.4999 - val_loss: 0.1386 - val_accuracy: 2.5000oss: 0.693 - ETA: 0s - loss: 0.6931 - accuracy: 0.\n",
      "Epoch 3/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5001 - val_loss: 0.1386 - val_accuracy: 2.3990\n",
      "Epoch 4/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5001 - val_loss: 0.1386 - val_accuracy: 2.5000\n",
      "Epoch 5/10\n",
      "2376/2376 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5001 ETA: 5s - loss: 0.6931 - accura - ETA: 0s - loss: 0.6931 - accura - ETA: 0s - loss: 0.6931 - ac - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5001 - val_loss: 0.1386 - val_accuracy: 2.5000\n",
      "Epoch 6/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5001 - val_loss: 0.1386 - val_accuracy: 2.5000accu - ETA: 2s - loss: 0.6931  - ETA: 2s - loss: 0.6931 - accuracy: 0.50 - ETA:  - ETA: 0s - loss: 0.6931 - accuracy: 0.\n",
      "Epoch 7/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.1386 - val_accuracy: 2.5000 ETA: 3s - loss: - ETA: 1s - loss: 0.6931 - accuracy: 0.50 - ETA: 1s - loss: 0.6931 - accuracy - ETA: 0s - loss: 0.6931 - accuracy:  - ETA: 0s - loss: 0.693 - ETA: 0s - loss: 0.6931 - accuracy - ETA: 0s - loss: 0.6931 - accuracy: 0.\n",
      "Epoch 8/10\n",
      "2376/2376 [==============================] - 8s 3ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.1386 - val_accuracy: 2.5000 0.6931 - \n",
      "Epoch 9/10\n",
      "2376/2376 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.4999 ETA: 5s - loss: - ETA: 4s - los - ETA: 1s - loss: 0.6931 - accuracy: 0.50 - ETA: 0s - loss: - 7s 3ms/step - loss: 0.6931 - accuracy: 0.4999 - val_loss: 0.1386 - val_accuracy: 2.5316\n",
      "Epoch 10/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5002 - val_loss: 0.1386 - val_accuracy: 2.5000cura\n",
      "########\\\\\\\\3////########\n",
      "########////4\\\\\\\\########\n",
      "Train on 2376 samples, validate on 792 samples\n",
      "Epoch 1/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.1386 - val_accuracy: 2.5000 loss: 0.6931 - ac - ETA: 5s - loss: 0.6931 -  - ETA: 5s - loss: - ETA: 4s - loss: 0.6931 - accuracy: 0. - ETA: 4s - loss: 0.6931 - accuracy - E - ETA - ETA: 2s - loss: 0.6931 - accu - ETA: 2s - loss: 0 - ETA: 1s - loss: - ETA: 0s - loss: 0.6 - ETA: 0s - loss: 0.6931 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accura\n",
      "Epoch 2/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5001 - val_loss: 0.1386 - val_accuracy: 2.4116curacy: 0.50 - ETA: 4s - loss: 0.693 - ETA: 4s - loss: 0.693 - ETA: 3s - loss: - ETA: 3s - - ETA: 2s - loss: - ETA: 1s - loss: 0.6931 - ac - ETA: 1s - loss: 0.6931 -  - ETA: 0s - loss: 0.6931 - accu - ETA: 0s - loss: 0\n",
      "Epoch 3/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5001 - val_loss: 0.1386 - val_accuracy: 2.5000\n",
      "Epoch 4/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.1386 - val_accuracy: 2.5126 - ETA: 4s - loss: 0.6931 -  - ETA: 4s - loss: 0.6931 -  - ETA: 3s - - ETA: 2s - los - ETA: 2s - loss: 0.6931 - accuracy:  - ETA: 2s - loss: 0.6931 - accuracy: 0. - ETA:  - E\n",
      "Epoch 5/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.1386 - val_accuracy: 2.3927TA: 0s - loss: 0\n",
      "Epoch 6/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5047 - val_loss: 0.1386 - val_accuracy: 2.3422 loss: 0.693 - ETA: 0s - loss: 0.6931 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accuracy: 0. - ETA: 0s - loss: 0.6931 - accuracy: 0.\n",
      "Epoch 7/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6931 - accuracy: 0.5528 - val_loss: 0.1386 - val_accuracy: 2.5000: 5s - loss: 0.6931 - accuracy - ETA: 5s - loss: 0.6931 - ac - ETA: 5s - los - ETA: 4s - loss: 0.6931 - accuracy: 0. - ETA: 4s - l - ETA: 1s - loss: 0.693 - ETA: 0s - loss:\n",
      "Epoch 8/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.6930 - accuracy: 0.5811 - val_loss: 0.1386 - val_accuracy: 2.4874\n",
      "Epoch 9/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.5539 - accuracy: 0.6950 - val_loss: 0.0327 - val_accuracy: 4.7854 0.5566 - accuracy: 0.\n",
      "Epoch 10/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.1436 - accuracy: 0.9505 - val_loss: 0.0180 - val_accuracy: 4.8737: 2s - loss: 0.1 - ETA: 2s - loss: 0.1553  - ETA: 1s - - E\n",
      "########\\\\\\\\4////########\n",
      "########////5\\\\\\\\########\n",
      "Train on 2376 samples, validate on 792 samples\n",
      "Epoch 1/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.1061 - accuracy: 0.9650 - val_loss: 0.0193 - val_accuracy: 4.8611 loss: 0.1088 - accuracy:  - ETA: 2s - loss: 0.1087 - accuracy: 0. - ETA: 2s - loss: 0.1 - ETA: 2s - loss: 0.1080 - accuracy - ETA: 2s - loss: 0.1079 - ac - ETA: 1s - l - ETA: 0s - loss: 0.1068  - ETA: 0s - loss: 0.106\n",
      "Epoch 2/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0992 - accuracy: 0.9667 - val_loss: 0.0199 - val_accuracy: 4.8548\n",
      "Epoch 3/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0958 - accuracy: 0.9680 - val_loss: 0.0200 - val_accuracy: 4.8674oss: 0.0969 - accu - ETA: 6s - loss: 0.0970 - accuracy:  - ETA: 4s - l - ETA: 2s - loss: 0.0 - ETA:  - ETA: 1s - loss: 0.0960 - accuracy:  - ETA: 1s - loss: 0.0960 - accura - ETA: 1s - loss: 0.0960 - accuracy:  - ETA: 0s\n",
      "Epoch 4/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0942 - accuracy: 0.9689 - val_loss: 0.0201 - val_accuracy: 4.86745 - accuracy: 0. - ETA: 2s - loss: 0.0945 - accuracy - ETA: 2s - loss: 0.0944 -  - - ETA: 1s - loss: 0.0 - ETA: 0s - loss: 0.0942 - accuracy - ETA: 0s - loss: 0.094\n",
      "Epoch 5/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0929 - accuracy: 0.9695 - val_loss: 0.0204 - val_accuracy: 4.8674 - loss: 0.0938 - accura - ETA: 5s - loss: 0.0934 - accuracy: 0. - ETA: 4s - loss: 0.093 - ETA: 4s - loss: 0.0933 - accu - ETA:  - ETA: 2s - loss: 0.0930 - accura - ETA: 1s - l - ETA:  - ETA: 0s - loss: 0.0929 - accuracy: 0.\n",
      "Epoch 6/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0921 - accuracy: 0.9699 - val_loss: 0.0203 - val_accuracy: 4.8611TA: 5s - - ETA: 4s - - ETA:  - ETA: 0s - loss: 0.0921 - \n",
      "Epoch 7/10\n",
      "2376/2376 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9701 ETA: 0s - loss: 0.0914 - accuracy - 7s 3ms/step - loss: 0.0914 - accuracy: 0.9701 - val_loss: 0.0212 - val_accuracy: 4.8674\n",
      "Epoch 8/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0908 - accuracy: 0.9703 - val_loss: 0.0212 - val_accuracy: 4.8674s: 0.0910 - accu\n",
      "Epoch 9/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0904 - accuracy: 0.9704 - val_loss: 0.0215 - val_accuracy: 4.8674\n",
      "Epoch 10/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0900 - accuracy: 0.9705 - val_loss: 0.0209 - val_accuracy: 4.86744s - ETA: 2s - loss: 0 - ETA: 2s - loss: 0.0 - ETA: 0s - loss: 0.0\n",
      "########\\\\\\\\5////########\n",
      "########////6\\\\\\\\########\n",
      "Train on 2376 samples, validate on 792 samples\n",
      "Epoch 1/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0897 - accuracy: 0.9706 - val_loss: 0.0213 - val_accuracy: 4.8737\n",
      "Epoch 2/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0893 - accuracy: 0.9706 - val_loss: 0.0222 - val_accuracy: 4.8674\n",
      "Epoch 3/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0891 - accuracy: 0.9707 - val_loss: 0.0215 - val_accuracy: 4.8737\n",
      "Epoch 4/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0888 - accuracy: 0.9708 - val_loss: 0.0222 - val_accuracy: 4.8737\n",
      "Epoch 5/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0887 - accuracy: 0.9708 - val_loss: 0.0224 - val_accuracy: 4.8674\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0884 - accuracy: 0.9709 - val_loss: 0.0224 - val_accuracy: 4.8674\n",
      "Epoch 7/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0883 - accuracy: 0.9710 - val_loss: 0.0220 - val_accuracy: 4.8737 ETA: 0s - loss: 0.0883 - accura - ETA: 0s - loss: 0.0883 - accuracy: 0.\n",
      "Epoch 8/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0881 - accuracy: 0.9710 - val_loss: 0.0222 - val_accuracy: 4.86745s - l - ETA: 3s - ETA: 2s - loss: 0.0881 -  - E - ETA: 1s - loss: 0.0881 - accuracy - ETA: 1s - l - ETA: 0s - loss: 0.0881 - accura\n",
      "Epoch 9/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0879 - accuracy: 0.9711 - val_loss: 0.0225 - val_accuracy: 4.8674\n",
      "Epoch 10/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0877 - accuracy: 0.9712 - val_loss: 0.0226 - val_accuracy: 4.8674\n",
      "########\\\\\\\\6////########\n",
      "########////7\\\\\\\\########\n",
      "Train on 2376 samples, validate on 792 samples\n",
      "Epoch 1/10\n",
      "2376/2376 [==============================] - ETA: 0s - loss: 0.0876 - accuracy: 0.9712 ETA: 5s - loss: 0.0877 - accu - 7s 3ms/step - loss: 0.0876 - accuracy: 0.9712 - val_loss: 0.0225 - val_accuracy: 4.8674\n",
      "Epoch 2/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0875 - accuracy: 0.9712 - val_loss: 0.0224 - val_accuracy: 4.8611\n",
      "Epoch 3/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0873 - accuracy: 0.9712 - val_loss: 0.0224 - val_accuracy: 4.8611accura\n",
      "Epoch 4/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0872 - accuracy: 0.9712 - val_loss: 0.0224 - val_accuracy: 4.8611oss: 0.0873 - ac - ETA: 4s - ETA - ETA: 2s - loss: - ETA: 1s - loss: 0.0872 - accuracy: 0.97 - ETA: 1s - loss: 0.0872 -  - ETA: 0s - loss: 0.0872 - accuracy - ETA: 0s - loss: 0.0872 - \n",
      "Epoch 5/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0869 - accuracy: 0.9712 - val_loss: 0.0225 - val_accuracy: 4.8611ra - ETA: 5s - loss: 0.0869 - accuracy:  - ETA: 5s - loss: 0.0869 -  - ETA: 5s - loss: 0.0869 - accuracy: 0. - ETA: 4s - loss: 0.0869  - ETA: 4s - loss: 0.0 - ETA: 3s - loss: 0.087 - ETA: 3s - loss: 0.0870  - ETA: 0s - loss: 0.0870  - ETA: 0s - loss: 0.0869 - accuracy:  - ETA: 0s - loss: 0.0869 - accuracy: 0.\n",
      "Epoch 6/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0867 - accuracy: 0.9713 - val_loss: 0.0229 - val_accuracy: 4.8611 loss: 0.0868 - accu - ETA: 0s - loss: 0.086\n",
      "Epoch 7/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0863 - accuracy: 0.9715 - val_loss: 0.0230 - val_accuracy: 4.8548\n",
      "Epoch 8/10\n",
      "2376/2376 [==============================] - ETA: 0s - loss: 0.0860 - accuracy: 0.9717 ETA: 6s - l - ETA: 5s - loss: 0.0862 - accura - ETA: 5s - - ETA: 4s - ETA: 3s - loss: - ETA: 3s - loss: 0.0861 - accuracy: 0.97 - ETA: 3s - loss: 0.0861 - ac - ETA: 2s - loss: 0.0861  - ETA: 2s - loss: 0.0861 - accuracy - ETA: 2s - loss: 0.0861 - accuracy: 0.97 - ETA: 2s - los - ETA: 1s - loss: 0.0860 - accuracy - ETA: 1s - loss: 0.0860 - accuracy - ETA: 0s - loss: 0.0860 -  - ETA: 0s - loss: 0 - 7s 3ms/step - loss: 0.0860 - accuracy: 0.9717 - val_loss: 0.0232 - val_accuracy: 4.8548\n",
      "Epoch 9/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0855 - accuracy: 0.9719 - val_loss: 0.0231 - val_accuracy: 4.8548.0855 - accura\n",
      "Epoch 10/10\n",
      "2376/2376 [==============================] - 7s 3ms/step - loss: 0.0849 - accuracy: 0.9719 - val_loss: 0.0226 - val_accuracy: 4.8548\n",
      "########\\\\\\\\7////########\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1dbf0ccfd68>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model('voicegender')\n",
    "model.summary()\n",
    "trainset, testset = load_dataset('voice.csv')\n",
    "history = train_model(model, int(input('Введите количество эпох обучения: ')), \\\n",
    " int(input('Введите количество циклов обучения: ')), trainset, testset)\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "male\t[✅]\n",
      "male\t[✅]\n",
      "male\t[✅]\n",
      "male\t[✅]\n",
      "male\t[✅]\n",
      "male\t[✅]\n",
      "male\t[✅]\n",
      "male\t[✅]\n",
      "male\t[✅]\n",
      "male\t[✅]\n",
      "male\t[✅]\n",
      "male\t[✅]\n",
      "male\t[✅]\n",
      "male\t[✅]\n",
      "male\t[✅]\n",
      "female\t[✅]\n",
      "female\t[✅]\n",
      "female\t[✅]\n",
      "female\t[✅]\n",
      "female\t[✅]\n",
      "female\t[✅]\n",
      "female\t[✅]\n",
      "female\t[✅]\n",
      "female\t[✅]\n",
      "female\t[✅]\n",
      "female\t[✅]\n",
      "female\t[✅]\n",
      "female\t[✅]\n",
      "female\t[✅]\n",
      "female\t[✅]\n"
     ]
    }
   ],
   "source": [
    "def getRandomData(dataset, gender, count=10):\n",
    "    selection = dataset[dataset['label'] == gender].drop(['label'], axis=1)\n",
    "    selection = getData(selection)\n",
    "    rows = selection.shape[0]\n",
    "    indexes = [random.randint(0, rows) for t in range(count)]\n",
    "    return selection.iloc[indexes].values.tolist()\n",
    "\n",
    "model = load_model('voicegender')\n",
    "dataset = pd.DataFrame(pd.read_csv('voice.csv', index_col=1))\n",
    "testset = getRandomData(dataset, 'male', 15) + getRandomData(dataset, 'female', 15)\n",
    "print(len(testset))\n",
    "i = 0\n",
    "correct = False\n",
    "for data in testset:\n",
    "    income = np.array(data).reshape(1,-1)\n",
    "    male, female = model.predict(income)[0]\n",
    "    \n",
    "    result = 'male' if male > female else 'female'\n",
    "    correct = (i < 15 and result == 'male') or (i >= 15 and result == 'female')\n",
    "    sign = '✅' if correct else '❌'\n",
    "    \n",
    "    print(f'{result}\\t[{sign}]')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
